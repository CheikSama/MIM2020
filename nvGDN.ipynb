{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mon_env_gdn\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib as plt \n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import spacy\n",
    "import tweepy as tw\n",
    "os.chdir(r\"C:\\Users\\csamassa\\Desktop\\Mémoire\\Nouveau GDN\")\n",
    "#os.chdir(r\"C:\\Users\\Cheik\\Desktop\\mémoire\")\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 20.08816432952881 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "demo=pd.read_csv(\"DEMOCRATIE_ET_CITOYENNETE.csv\", \n",
    "                 sep=\",\",\n",
    "                 #nrows=10,\n",
    "                 usecols=[0,10,2,11,13,14,16,17,19,20,22,23,25,26,27,29,30,31,32,33,34,35,36,37,38,39,40,42,43,44,45,46,47]\n",
    "                 ,dtype={\"authorZipCode\":object}\n",
    "                 )\n",
    "fisc=pd.read_csv(\"LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv\", \n",
    "                 sep=\",\",\n",
    "                 #nrows=10,\n",
    "                 usecols=[0,10,2,11,12,13,14,15,16,17,18]\n",
    "                 ,dtype={\"authorZipCode\":object}\n",
    "                 )\n",
    "eco=pd.read_csv(\"LA_TRANSITION_ECOLOGIQUE.csv\", \n",
    "                sep=\",\",\n",
    "                #nrows=10,               \n",
    "                usecols=[0,10,2,11,12,14,16,17,18,20,22,23,24,25,26]\n",
    "                ,dtype={\"authorZipCode\":object}\n",
    "                )\n",
    "                \n",
    "org=pd.read_csv(\"ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv\", \n",
    "                sep=\",\",\n",
    "               #nrows=10,\n",
    "                usecols=[0,10,2,11,13,15,16,19,20,21,24,25,27,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]\n",
    "                ,dtype={\"authorZipCode\":object}\n",
    "               )\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 12.559457778930664 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# On rajoute une colonne comportant le thème de chaque question\n",
    "demo.insert(column=\"Thème\",value=\"DEMOCRATIE ET CITOYENNETE\",loc=3)\n",
    "fisc.insert(column=\"Thème\",value=\"LA FISCALITE ET LES DEPENSES PUBLIQUES\",loc=3)\n",
    "eco.insert(column=\"Thème\",value=\"LA TRANSITION ECOLOGIQUE\",loc=3)\n",
    "org.insert(column=\"Thème\",value=\"ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES\",loc=3)\n",
    "\n",
    "\n",
    "# On nettoie le début des questions\n",
    "def clean_question(df):\n",
    "    colonnes=df.columns\n",
    "    colonnes1=[re.sub(pattern=r\"\\bQ[A-Za-z0-9]+\\s+\\-\\s\",repl='',string=nom) for nom in colonnes]\n",
    "    return(colonnes1)\n",
    "\n",
    "\n",
    "\n",
    "# On applique tt en même temps\n",
    "demo.columns,fisc.columns,eco.columns,org.columns=clean_question(demo),clean_question(fisc),clean_question(eco),clean_question(org)\n",
    "col1=[\"id\", \"authorZipCode\",\"Thème\"]\n",
    "\n",
    "def empiller(df):\n",
    "    stack_0=df.loc[:, ~df.columns.isin(col1)].stack(dropna=False) # prend les QO (toutes les questions sauf celles de col1)\n",
    "    stack_1=stack_0.reset_index()                                 # On supp l'index pour avoir le level 0 pour la future jointure\n",
    "    stack_2=stack_1.merge(df[col1],left_on=\"level_0\",right_index=True,how=\"left\")\n",
    "    stack_2.columns=[\"idx_0\",\"Question\",\"Réponse\",\"id\",\"authorZipCode\",\"Thème\"] #idx_0 c'est le numéro de la ligne dans le fichier original de chaque thème \n",
    "    stack_2.dropna(inplace=True)\n",
    "    return stack_2\n",
    "\n",
    "\n",
    "demo_1,fisc_1,eco_1,org_1=empiller(demo),empiller(fisc),empiller(eco),empiller(org)\n",
    "\n",
    "del(demo,fisc,eco,org)# On supp les variables inutiles de l'environnement\n",
    "\n",
    "contributions=pd.concat([demo_1,fisc_1,eco_1,org_1],axis=0)   #On met tout dans un même df\n",
    " \n",
    "contributions.reset_index(drop=True,inplace=True)\n",
    "\n",
    "del(demo_1,fisc_1,eco_1,org_1,col1)\n",
    "\n",
    "contributions.drop(columns=\"id\", inplace=True) # On supp la colonne (on a qu'à utiliser idx_0 si on veut la trace des contributions)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx_0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Réponse</th>\n",
       "      <th>authorZipCode</th>\n",
       "      <th>Thème</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>title</td>\n",
       "      <td>Les augmentations de rémunérations</td>\n",
       "      <td>79190</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>En qui faites-vous le plus confiance pour vous...</td>\n",
       "      <td>Le citoyen</td>\n",
       "      <td>79190</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Y a-t-il d'autres points sur la démocratie et ...</td>\n",
       "      <td>Afin d’éviter de creuser les inégalités ne plu...</td>\n",
       "      <td>79190</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>title</td>\n",
       "      <td>rénover l'enquête publique pour en faire un vr...</td>\n",
       "      <td>01800</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>En qui faites-vous le plus confiance pour vous...</td>\n",
       "      <td>Un instrument de démocratie locale à modernise...</td>\n",
       "      <td>01800</td>\n",
       "      <td>DEMOCRATIE ET CITOYENNETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963582</th>\n",
       "      <td>111952</td>\n",
       "      <td>Quand vous pensez à l'évolution des services p...</td>\n",
       "      <td>Je n'en vois pas beaucoup, mais je constate pa...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963583</th>\n",
       "      <td>111952</td>\n",
       "      <td>Quels sont les services publics qui doivent le...</td>\n",
       "      <td>La SNCF , la renationalisation et en refaire u...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963584</th>\n",
       "      <td>111952</td>\n",
       "      <td>Si oui, comment ?</td>\n",
       "      <td>En les laissant faire preuve de bon sens avec ...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963585</th>\n",
       "      <td>111952</td>\n",
       "      <td>Si oui, comment ?</td>\n",
       "      <td>En simplifiant les normes tout en maintenant u...</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963586</th>\n",
       "      <td>111952</td>\n",
       "      <td>Si vous avez été amené à demander un rembourse...</td>\n",
       "      <td>Des relevés de remboursement papier trop rares</td>\n",
       "      <td>50260</td>\n",
       "      <td>ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5963587 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx_0                                           Question  \\\n",
       "0             0                                              title   \n",
       "1             0  En qui faites-vous le plus confiance pour vous...   \n",
       "2             0  Y a-t-il d'autres points sur la démocratie et ...   \n",
       "3             1                                              title   \n",
       "4             1  En qui faites-vous le plus confiance pour vous...   \n",
       "...         ...                                                ...   \n",
       "5963582  111952  Quand vous pensez à l'évolution des services p...   \n",
       "5963583  111952  Quels sont les services publics qui doivent le...   \n",
       "5963584  111952                                  Si oui, comment ?   \n",
       "5963585  111952                                  Si oui, comment ?   \n",
       "5963586  111952  Si vous avez été amené à demander un rembourse...   \n",
       "\n",
       "                                                   Réponse authorZipCode  \\\n",
       "0                       Les augmentations de rémunérations         79190   \n",
       "1                                               Le citoyen         79190   \n",
       "2        Afin d’éviter de creuser les inégalités ne plu...         79190   \n",
       "3        rénover l'enquête publique pour en faire un vr...         01800   \n",
       "4        Un instrument de démocratie locale à modernise...         01800   \n",
       "...                                                    ...           ...   \n",
       "5963582  Je n'en vois pas beaucoup, mais je constate pa...         50260   \n",
       "5963583  La SNCF , la renationalisation et en refaire u...         50260   \n",
       "5963584  En les laissant faire preuve de bon sens avec ...         50260   \n",
       "5963585  En simplifiant les normes tout en maintenant u...         50260   \n",
       "5963586     Des relevés de remboursement papier trop rares         50260   \n",
       "\n",
       "                                                    Thème  \n",
       "0                               DEMOCRATIE ET CITOYENNETE  \n",
       "1                               DEMOCRATIE ET CITOYENNETE  \n",
       "2                               DEMOCRATIE ET CITOYENNETE  \n",
       "3                               DEMOCRATIE ET CITOYENNETE  \n",
       "4                               DEMOCRATIE ET CITOYENNETE  \n",
       "...                                                   ...  \n",
       "5963582  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963583  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963584  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963585  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "5963586  ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES  \n",
       "\n",
       "[5963587 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "\n",
    "#contributions=pd.read_csv('contributions.csv',dtype={\"authorZipCode\":object})\n",
    "\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def compte_group(df,nom,fichier=None,export=False):\n",
    "    resultat=df.loc[df[\"Réponse\"].str.contains(nom,case=False, regex=True)]\n",
    "    ###On va mettre chaque thème dans une feuille différente\n",
    "    ##D'abord on filtre les résultats par thèmes\n",
    "    resultat[\"nb_occurr\"]=resultat.loc[:,\"Réponse\"].str.count(pat=nom, flags=re.I)\n",
    "    if export==True:\n",
    "        resultat_1=resultat.loc[resultat[\"Thème\"]==\"DEMOCRATIE ET CITOYENNETE\"]\n",
    "        resultat_2=resultat.loc[resultat[\"Thème\"]==\"LA FISCALITE ET LES DEPENSES PUBLIQUES\"]\n",
    "        resultat_3=resultat.loc[resultat[\"Thème\"]==\"LA TRANSITION ECOLOGIQUE\"]\n",
    "        resultat_4=resultat.loc[resultat[\"Thème\"]==\"ORGANISATION DE L'ETAT ET DES SERVICES PUBLIQUES\"] # je sais que c'est public mais changer ça est trop chiant à faire pzrce que faudra changer le nom des thèmes (ou y'a la faute aussi)\n",
    "    \n",
    "        writer = pd.ExcelWriter(fichier+'.xlsx', engine='xlsxwriter')\n",
    "    \n",
    "        resultat_1.to_excel(writer, sheet_name=\"DEMOCRATIE\")\n",
    "        resultat_2.to_excel(writer, sheet_name=\"FISCALITE\")\n",
    "        resultat_3.to_excel(writer, sheet_name=\"TRANSITION_ECOLOGIQUE\")\n",
    "        resultat_4.to_excel(writer, sheet_name=\"ORGANISATION_DE_LETAT\") \n",
    "    \n",
    "        writer.save()\n",
    "    return(resultat)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va prendre un exemple pour essayer notre code: toutes les contributions du GDN qui contiennent les mots pandémie, épidémie sras, coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 71.59719014167786 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csamassa\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "corona=compte_group(contributions,r\"\\bpand[e-é]mi[a-z]\\b|\\b[e-é]pid[e-é]mie\\b|\\bsras\\b|\\bcoronavirus\\b\")\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#DF series.apply est plus rapide que df.apply\n",
    "\n",
    "#On ne  prend que les contributions de plus d'une phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.44719815254211426 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "aleo=corona[\"Réponse\"].apply(nltk.sent_tokenize)\n",
    "essai=pd.DataFrame(aleo.apply(pd.Series).stack()).reset_index()#la fonction series fait que le séparateur devient la virgule entre chaque éléments de la liste, le stack fait qu'on les empile, on a un multi index où chaque index de level 0= numéro de la contribution, level 1= numéro de la phrase dans la contribution\n",
    "\n",
    "#On ajoute le nombre de phrases de chaque contributions\n",
    "essai_gpby=essai.groupby(by=\"level_0\",as_index=False)[0].count()\n",
    "\n",
    "essai=essai.merge(essai_gpby,on=\"level_0\")\n",
    "\n",
    "\n",
    "essai.columns=[\"level_0\",\"nb_sent\",\"sent\",\"nb_sent_total\"]\n",
    "contain=essai.loc[essai[\"sent\"].str.contains(r\"\\bpand[e-é]mi[a-z]\\b|\\b[e-é]pid[e-é]mie\\b|\\bsras\\b|\\bcoronavirus\\b\",regex=True,case=False)]\n",
    "\n",
    "# On met dans un dictionnaire l'index de chaque phrase contenant notre pattern\n",
    "dico_lvl_contain=dict(zip(contain.index,zip(contain[\"nb_sent\"],contain[\"nb_sent_total\"])))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#DF series.apply est plus rapide que df.apply\n",
    "\n",
    "#On ne  prend que les contributions de plus d'une phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque phrase qui contient les mots qu'on veut, on prend celle d'avant et celle d'après. \n",
    "Si jamais la phrase qui contient les patterns est la première, python va renvoyer une erreur lorsqu'on va lui demander \n",
    "de prendre la phrase d'avant vu qu'il n'y a rien avant, viceversa si c'est après. \n",
    "Pour résoudre cela, on va dire à python d'exclure la première lorsqu'on lui demandera de prendre les phrases x-1 et la dernière phrase quand on lui demande de prendre les phrases x+1 (encore une fois si y'a rien après x on va avoir une erreur index out of bounds). \n",
    "On a selectionné dans une autre opérationles phrases qui contiennent le pattern, cette opération a pour but de prendre x-1 et x+1 pas x. \n",
    "Si la première phrase contient le pattern, python ne va prendre que celle qui la suit, et si dernière phrase contient le pattern, python ne va prendre que celle qui la précède, \n",
    "Si la première et la deuxième phrase contiennent le patern, python va prendre celle qui la suit (x+1), et la ligne suivante python va de nouveau prendre la première (x-1, qui contient le pattern),  et la troisème (x+1), la deuxième étant x sur cette ligne (x+1 la ligne précédente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.05900001525878906 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "idx_avant=[[k-1,k,v] for (k,v) in dico_lvl_contain.items() if v[0]>0] #prend la phrase d'avant si la phrase  (celle qui matche le pattern) n'est pas la première de la contribution, donc d'index supérieur à 0\n",
    "phrase_avant=essai.loc[[x[0] for x in idx_avant],]\n",
    "phrase_avant[\"id_x\"]=essai.loc[[x[1] for x in idx_avant],].index # On ajoute l'index des phrases qui contiennent la phrase du patern(celle d'après du coup si répond aux conditions du for)\n",
    "phrase_avant.columns=[\"level_0\",\"nb_sent_av\", \"sent_av\",\"nb_sent_total\",\"id_x\"]\n",
    "\n",
    "idx_apres=[[k+1,k,v] for (k,v) in dico_lvl_contain.items() if v[0]<(v[1]-1)] # # prend la phrase d'après si la phrase n'est pas la dernière de la contribution, donc d'index inférieure au nombre de phrases dans la contribution. on ajoute moins 1 parce que v[1] c'est la taille par ex 20 et v[0] c'est l'index (0:19) donc faut mettre v[1]-1 pour que ça corresponde\n",
    "\n",
    "phrase_apres=essai.loc[[x[0] for x in idx_apres],]\n",
    "phrase_apres[\"id_x\"]=essai.loc[[x[1] for x in idx_apres],].index #On ajoute l'index des phrases du pattern (celle d'avant du coup si matche les conditions)\n",
    "phrase_apres.columns=[\"level_0\",\"nb_sent_ap\", \"sent_ap\",\"nb_sent_total\",\"id_x\"]\n",
    "\n",
    "contain=contain.rename_axis('id_x').reset_index() #On ajoute la colonne de l'id pour la jointure\n",
    "essai_final=contain.merge(phrase_avant, on=\"id_x\", how=\"outer\").merge(phrase_apres,on=\"id_x\",how=\"outer\") #On fait une union des trois dataframes\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_av</th>\n",
       "      <th>sent</th>\n",
       "      <th>sent_ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Que tous les profs les informent de l’existenc...</td>\n",
       "      <td>Lutte contre EPIDEMIE sida, diminution nb d’av...</td>\n",
       "      <td>Tous les enfants ont les mêmes droits.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qui est une injustice sociale exorbitante au r...</td>\n",
       "      <td>Prévue pour faire face aux risques d'épidémie ...</td>\n",
       "      <td>Elle donne donc lieu à une immigration sanitai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Les gens ne partiraient pas si leur vie n'étai...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensuite la langue, les lois, les DEVOIRS.</td>\n",
       "      <td>Si leur dossier est recevable parce que la gue...</td>\n",
       "      <td>Si comme dirait le Premier Ministre Néozélanda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>Il faut pourvoir distinguer les asiles d'urgen...</td>\n",
       "      <td>Pour les autres, il faut faire appliquer des c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Le rétablissement du jour de carence suffirait...</td>\n",
       "      <td>le taux d’absentéisme pour raisons de santé, q...</td>\n",
       "      <td>le problème tient au statut très avantageux de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>ça ne se passe pas si aml que ça, sauf que de ...</td>\n",
       "      <td>aussi des congés, une épidémie et des services...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Les cabinets de médecins sont débordés et la p...</td>\n",
       "      <td>Lorsqu’on a besoin d’une consultation simple (...</td>\n",
       "      <td>Il s’agirait d’un niveau « 1 » qui desorgorger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td></td>\n",
       "      <td>Les pénuries de médicaments de plus en plus fr...</td>\n",
       "      <td>Il faut assurer notre indépendance dans ce dom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Une chaine crypté d'accès à un espace concitoy...</td>\n",
       "      <td>les services centralisé de tous la Républiques...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent_av  \\\n",
       "0    Que tous les profs les informent de l’existenc...   \n",
       "1    qui est une injustice sociale exorbitante au r...   \n",
       "2                                                        \n",
       "3            Ensuite la langue, les lois, les DEVOIRS.   \n",
       "4                                                        \n",
       "..                                                 ...   \n",
       "199  Le rétablissement du jour de carence suffirait...   \n",
       "200  ça ne se passe pas si aml que ça, sauf que de ...   \n",
       "201  Les cabinets de médecins sont débordés et la p...   \n",
       "202                                                      \n",
       "203  Une chaine crypté d'accès à un espace concitoy...   \n",
       "\n",
       "                                                  sent  \\\n",
       "0    Lutte contre EPIDEMIE sida, diminution nb d’av...   \n",
       "1    Prévue pour faire face aux risques d'épidémie ...   \n",
       "2    Les gens ne partiraient pas si leur vie n'étai...   \n",
       "3    Si leur dossier est recevable parce que la gue...   \n",
       "4    Il faut pourvoir distinguer les asiles d'urgen...   \n",
       "..                                                 ...   \n",
       "199  le taux d’absentéisme pour raisons de santé, q...   \n",
       "200  aussi des congés, une épidémie et des services...   \n",
       "201  Lorsqu’on a besoin d’une consultation simple (...   \n",
       "202  Les pénuries de médicaments de plus en plus fr...   \n",
       "203  les services centralisé de tous la Républiques...   \n",
       "\n",
       "                                               sent_ap  \n",
       "0               Tous les enfants ont les mêmes droits.  \n",
       "1    Elle donne donc lieu à une immigration sanitai...  \n",
       "2                                                       \n",
       "3    Si comme dirait le Premier Ministre Néozélanda...  \n",
       "4    Pour les autres, il faut faire appliquer des c...  \n",
       "..                                                 ...  \n",
       "199  le problème tient au statut très avantageux de...  \n",
       "200                                                     \n",
       "201  Il s’agirait d’un niveau « 1 » qui desorgorger...  \n",
       "202  Il faut assurer notre indépendance dans ce dom...  \n",
       "203                                                     \n",
       "\n",
       "[204 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essai_final.loc[:,[\"sent_av\",\"sent\",\"sent_ap\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai_final['sent_av']=essai_final['sent_av'].astype(str).replace('nan','')\n",
    "essai_final['sent_ap']=essai_final['sent_ap'].astype(str).replace('nan','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai_final[\"phrase_av_ap\"]=essai_final[['sent_av', 'sent','sent_ap']].apply(lambda x: ''.join(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords et punct removal perso, mais on va plutôt utiliser celui de Spacy\n",
    "'''\n",
    "def stop_words(liste=False):\n",
    "    stopwords=pd.read_csv(\"stopwords.csv\",sep=';')\n",
    "    nvx_stpwds=pd.DataFrame(liste,dtype=str)\n",
    "    stopwords=pd.concat([stopwords[\"a\"],nvx_stpwds[0]],axis=0,ignore_index=True)\n",
    "    stopwords=pd.concat([stopwords,stopwords.str.upper(),stopwords.str.capitalize()],axis=0, ignore_index=True)\n",
    "    \n",
    "    return(stopwords.tolist())\n",
    "stopwords=stop_words(['oui','non','faut','faudrait','une','qu','il'])\n",
    "\n",
    "\n",
    "essai_final[\"text_processed\"] = essai_final[\"phrase_av_ap\"].map(lambda x: re.sub('[,\\.!?:]', ' ', x))\n",
    "essai_final[\"text_processed\"]=essai_final[\"text_processed\"].map(lambda x:' '.join([word for word in x.split() if word not in (stopwords)]))#On enlève les stopwords\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import fr_core_news_sm\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "nlp = fr_core_news_sm.load()\n",
    "#stop_list=[\"oui\",\"non\",\"faut\",\"faudrait\",\"une\",\"qu\",\"il\"]\n",
    "nlp = fr_core_news_sm.load()\n",
    "\n",
    "#nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True\n",
    "    \n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'ville']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Paris est une ville très chère.\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Defaults',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_format_docs_and_golds',\n",
       " '_meta',\n",
       " '_multiprocessing_pipe',\n",
       " '_optimizer',\n",
       " '_path',\n",
       " 'add_pipe',\n",
       " 'begin_training',\n",
       " 'create_pipe',\n",
       " 'disable_pipes',\n",
       " 'entity',\n",
       " 'evaluate',\n",
       " 'factories',\n",
       " 'from_bytes',\n",
       " 'from_disk',\n",
       " 'get_pipe',\n",
       " 'has_pipe',\n",
       " 'lang',\n",
       " 'linker',\n",
       " 'make_doc',\n",
       " 'matcher',\n",
       " 'max_length',\n",
       " 'meta',\n",
       " 'parser',\n",
       " 'path',\n",
       " 'pipe',\n",
       " 'pipe_factories',\n",
       " 'pipe_labels',\n",
       " 'pipe_names',\n",
       " 'pipeline',\n",
       " 'preprocess_gold',\n",
       " 'rehearse',\n",
       " 'remove_pipe',\n",
       " 'rename_pipe',\n",
       " 'replace_pipe',\n",
       " 'resume_training',\n",
       " 'tagger',\n",
       " 'tensorizer',\n",
       " 'to_bytes',\n",
       " 'to_disk',\n",
       " 'tokenizer',\n",
       " 'update',\n",
       " 'use_params',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d95976c18647159eda61f220ce3477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_list = []\n",
    "entité=[]\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in tqdm(essai_final[\"phrase_av_ap\"]):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)    \n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['injustice',\n",
       " 'social',\n",
       " 'exorbitante',\n",
       " 'regard',\n",
       " 'situation',\n",
       " 'français',\n",
       " 'matière',\n",
       " 'santé',\n",
       " 'prévue',\n",
       " 'faire',\n",
       " 'face',\n",
       " 'risque',\n",
       " 'épidémie',\n",
       " 'véhiculer',\n",
       " 'migrant',\n",
       " 'attribuer',\n",
       " 'pathologie',\n",
       " 'aucunement',\n",
       " 'contagieux',\n",
       " 'donne',\n",
       " 'lieu',\n",
       " 'immigration',\n",
       " 'sanitaire',\n",
       " 'onéreux',\n",
       " 'communauté']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ent in pr.ents:\n",
    "        entité.append(ent.text, ent.start_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bd752f3c09a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'ents'"
     ]
    }
   ],
   "source": [
    "ents=[]\n",
    "for doc in tqdm(essai_final[\"phrase_av_ap\"]):\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        prc=nlp(ent)\n",
    "        prc.append(ent.text, ent.start_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['culte',\n",
       " 'pré-carré',\n",
       " 'n',\n",
       " 'héler',\n",
       " 'réserver',\n",
       " 'élite',\n",
       " '.c',\n",
       " 'pandémie',\n",
       " 'grave',\n",
       " 'vaccin',\n",
       " 'sérum',\n",
       " 'réseau',\n",
       " 'sociau',\n",
       " 'anonymat',\n",
       " 'détruire',\n",
       " 'société',\n",
       " 'exprimer',\n",
       " 'crainte',\n",
       " 'représaille',\n",
       " 'juridique',\n",
       " 'bon',\n",
       " 'nombre',\n",
       " 'faux',\n",
       " 'idée',\n",
       " 'nuisible',\n",
       " 'bêtise',\n",
       " 'illimiter']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionnaire = gensim.corpora.Dictionary(doc_list) # dictionnaire des tokens uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionnaire.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le mot 55 (\"faire\") aparait 1 fois.\n",
      "Le mot 72 (\"épidémie\") aparait 1 fois.\n",
      "Le mot 110 (\"exemple\") aparait 1 fois.\n",
      "Le mot 120 (\"vrai\") aparait 1 fois.\n",
      "Le mot 567 (\"attendre\") aparait 1 fois.\n",
      "Le mot 577 (\"remplacer\") aparait 1 fois.\n",
      "Le mot 1042 (\"prendre\") aparait 1 fois.\n",
      "Le mot 1125 (\"hiver\") aparait 1 fois.\n",
      "Le mot 1162 (\"maladie\") aparait 1 fois.\n",
      "Le mot 1309 (\"totalement\") aparait 1 fois.\n",
      "Le mot 1591 (\"plante\") aparait 1 fois.\n",
      "Le mot 1659 (\"local\") aparait 2 fois.\n",
      "Le mot 1711 (\"insecte\") aparait 1 fois.\n",
      "Le mot 1722 (\"biodiversité\") aparait 1 fois.\n",
      "Le mot 1728 (\"dengue\") aparait 1 fois.\n",
      "Le mot 1870 (\"flore\") aparait 1 fois.\n",
      "Le mot 1906 (\"Australie\") aparait 1 fois.\n",
      "Le mot 1907 (\"bloquer\") aparait 1 fois.\n",
      "Le mot 1908 (\"chikungunya?et\") aparait 1 fois.\n",
      "Le mot 1909 (\"chose?une\") aparait 1 fois.\n",
      "Le mot 1910 (\"exotique\") aparait 1 fois.\n",
      "Le mot 1911 (\"grosse\") aparait 1 fois.\n",
      "Le mot 1912 (\"importation\") aparait 1 fois.\n",
      "Le mot 1913 (\"moustique\") aparait 2 fois.\n",
      "Le mot 1914 (\"préserve\") aparait 1 fois.\n",
      "Le mot 1915 (\"ravageur\") aparait 1 fois.\n",
      "Le mot 1916 (\"survivre\") aparait 1 fois.\n",
      "Le mot 1917 (\"tigre\") aparait 1 fois.\n",
      "Le mot 1918 (\"vectrice\") aparait 1 fois.\n"
     ]
    }
   ],
   "source": [
    "doc_130 = corpus[130]\n",
    "for i in range(len(doc_130)):\n",
    "    print(\"Le mot {} (\\\"{}\\\") aparait {} fois.\".format(doc_130[i][0], \n",
    "                                               dictionnaire[doc_130[i][0]], \n",
    "doc_130[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionnaire,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"épidémie\" + 0.012*\"l\" + 0.009*\"faire\" + 0.009*\"d\" + 0.007*\"pouvoir\" '\n",
      "  '+ 0.007*\"n\" + 0.007*\"cas\" + 0.007*\"devoir\" + 0.005*\"  \" + 0.005*\"maladie\"'),\n",
      " (1,\n",
      "  '0.011*\"  \" + 0.011*\"épidémie\" + 0.007*\"gestion\" + 0.007*\"administratif\" + '\n",
      "  '0.007*\"n\" + 0.007*\"devoir\" + 0.007*\"faire\" + 0.006*\"l\" + 0.005*\"falloir\" + '\n",
      "  '0.005*\"pandémie\"'),\n",
      " (2,\n",
      "  '0.017*\"épidémie\" + 0.012*\"l\" + 0.010*\"  \" + 0.010*\"d\" + 0.007*\"faire\" + '\n",
      "  '0.005*\"n\" + 0.005*\"santé\" + 0.005*\"y\" + 0.004*\"falloir\" + '\n",
      "  '0.004*\"politique\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.05924481494731159),\n",
      " (1, 0.1321443107530509),\n",
      " (2, 0.1418129870198082),\n",
      " (3, 0.1418129870198082),\n",
      " (4, 0.1418129870198082),\n",
      " (5, 0.1321443107530509),\n",
      " (6, 0.1418129870198082),\n",
      " (7, 0.07805288262146544),\n",
      " (8, 0.1418129870198082),\n",
      " (9, 0.1321443107530509),\n",
      " (10, 0.1418129870198082),\n",
      " (11, 0.31128290243047163),\n",
      " (12, 0.1418129870198082),\n",
      " (13, 0.1321443107530509),\n",
      " (14, 0.1418129870198082),\n",
      " (15, 0.1048898431085945),\n",
      " (16, 0.1418129870198082),\n",
      " (17, 0.1418129870198082),\n",
      " (18, 0.12464470279943649),\n",
      " (19, 0.1418129870198082),\n",
      " (20, 0.2856635005255116),\n",
      " (21, 0.1418129870198082),\n",
      " (22, 0.1418129870198082),\n",
      " (23, 0.1321443107530509),\n",
      " (24, 0.1418129870198082),\n",
      " (25, 0.10134879271045097),\n",
      " (26, 0.1418129870198082),\n",
      " (27, 0.1321443107530509),\n",
      " (28, 0.2836259740396164),\n",
      " (29, 0.1418129870198082),\n",
      " (30, 0.1418129870198082),\n",
      " (31, 0.1418129870198082),\n",
      " (32, 0.1418129870198082),\n",
      " (33, 0.10011945646355355),\n",
      " (34, 0.1418129870198082),\n",
      " (35, 0.05079841497700909),\n",
      " (36, 0.1418129870198082),\n",
      " (37, 0.10884840066406534),\n",
      " (38, 0.12464470279943649),\n",
      " (39, 0.05155370280628277),\n",
      " (40, 0.1418129870198082),\n",
      " (41, 0.1418129870198082),\n",
      " (42, 0.1418129870198082),\n",
      " (43, 0.11333624307889702),\n",
      " (44, 0.11851707693082271),\n",
      " (45, 0.14671127664152459),\n",
      " (46, 0.06556504245452961),\n",
      " (47, 0.10884840066406534)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LDA_multicore\n",
    "lda_model = gensim.models.LdaMulticore(corpus, num_topics=10, id2word=dictionnaire, passes=2, workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.018*\"épidémie\" + 0.012*\"l\" + 0.009*\"pouvoir\" + 0.009*\"d\" + 0.008*\"cancer\" + 0.008*\"devoir\" + 0.008*\"faire\" + 0.007*\"\t \" + 0.007*\"grippe\" + 0.007*\"mettre\"\n",
      "Topic: 1 \n",
      "Words: 0.018*\"épidémie\" + 0.009*\"devoir\" + 0.009*\"  \" + 0.006*\"limiter\" + 0.006*\"chasse\" + 0.006*\"vie\" + 0.005*\"l\" + 0.005*\"aide\" + 0.005*\"n\" + 0.005*\"faire\"\n",
      "Topic: 2 \n",
      "Words: 0.021*\"épidémie\" + 0.010*\"l\" + 0.009*\"pouvoir\" + 0.008*\"faire\" + 0.008*\"falloir\" + 0.008*\"n\" + 0.007*\"pandémie\" + 0.006*\"vie\" + 0.006*\"y\" + 0.005*\"aller\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"  \" + 0.016*\"épidémie\" + 0.012*\"gestion\" + 0.011*\"administratif\" + 0.009*\"l\" + 0.009*\"santé\" + 0.008*\"faire\" + 0.008*\"social\" + 0.006*\"devoir\" + 0.005*\"n\"\n",
      "Topic: 4 \n",
      "Words: 0.032*\"d\" + 0.019*\"l\" + 0.016*\"épidémie\" + 0.009*\"n\" + 0.007*\"devoir\" + 0.007*\"qu\" + 0.006*\"faire\" + 0.006*\"enfant\" + 0.006*\"  \" + 0.005*\"médical\"\n",
      "Topic: 5 \n",
      "Words: 0.014*\"épidémie\" + 0.008*\"population\" + 0.008*\"n\" + 0.008*\"antibiotique\" + 0.006*\"France\" + 0.006*\"eau\" + 0.006*\"prendre\" + 0.005*\"politique\" + 0.005*\"changer\" + 0.005*\"culture\"\n",
      "Topic: 6 \n",
      "Words: 0.010*\"faire\" + 0.010*\"épidémie\" + 0.007*\"maladie\" + 0.006*\"n\" + 0.006*\"l\" + 0.005*\"   \" + 0.005*\"collectivité\" + 0.005*\"exemple\" + 0.005*\"pouvoir\" + 0.005*\"condition\"\n",
      "Topic: 7 \n",
      "Words: 0.023*\"l\" + 0.013*\"épidémie\" + 0.009*\"risque\" + 0.007*\"agriculture\" + 0.006*\"rappeler\" + 0.005*\"richesse\" + 0.005*\"population\" + 0.004*\"espèce\" + 0.004*\"campagne\" + 0.004*\"règle\"\n",
      "Topic: 8 \n",
      "Words: 0.022*\"  \" + 0.014*\"épidémie\" + 0.010*\"l\" + 0.009*\"falloir\" + 0.008*\"y\" + 0.008*\"faire\" + 0.008*\"devoir\" + 0.007*\"n\" + 0.006*\"cas\" + 0.004*\"société\"\n",
      "Topic: 9 \n",
      "Words: 0.016*\"épidémie\" + 0.014*\"faire\" + 0.010*\"cas\" + 0.008*\"produit\" + 0.008*\"n\" + 0.008*\"utiliser\" + 0.007*\"falloir\" + 0.007*\"devoir\" + 0.006*\"  \" + 0.005*\"risque\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9844813346862793\t \n",
      "Topic: 0.032*\"d\" + 0.019*\"l\" + 0.016*\"épidémie\" + 0.009*\"n\" + 0.007*\"devoir\"\n"
     ]
    }
   ],
   "source": [
    "#évaluation modèle\n",
    "for index, score in sorted(lda_model[corpus[50]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.002*\"faire\" + 0.002*\"vouloir\" + 0.002*\"impôt\" + 0.002*\"suicide\" + 0.002*\"contribuable\" + 0.002*\"provoquer\" + 0.002*\"gagne\" + 0.002*\"quitter\" + 0.002*\"crédit\" + 0.002*\"déjection\"\n",
      "Topic: 1 Word: 0.002*\"revoir\" + 0.002*\"allocation\" + 0.002*\"falloir\" + 0.002*\"médecin\" + 0.002*\"   \" + 0.002*\"jamais\" + 0.002*\"virus\" + 0.002*\"\t \" + 0.001*\"pognon\" + 0.001*\"dingue\"\n",
      "Topic: 2 Word: 0.004*\"cancer\" + 0.003*\"maladie\" + 0.002*\"chronique\" + 0.002*\"espèce\" + 0.002*\"toucher\" + 0.002*\"véritable\" + 0.002*\"chasse\" + 0.002*\"jeune\" + 0.002*\"animal\" + 0.002*\"apparition\"\n",
      "Topic: 3 Word: 0.003*\"devoir\" + 0.002*\"année\" + 0.002*\"  \" + 0.002*\"taxer\" + 0.002*\">\" + 0.002*\"=\" + 0.002*\"faire\" + 0.002*\"peur\" + 0.002*\"comportement\" + 0.002*\"source\"\n",
      "Topic: 4 Word: 0.003*\"falloir\" + 0.002*\"pouvoir\" + 0.002*\"cas\" + 0.002*\"risque\" + 0.002*\"entreprise\" + 0.002*\"médical\" + 0.002*\"l\" + 0.002*\"etat\" + 0.002*\"terme\" + 0.002*\"atteindre\"\n",
      "Topic: 5 Word: 0.002*\"l\" + 0.002*\"agriculture\" + 0.002*\"guerre\" + 0.002*\"nanti\" + 0.002*\"enclave\" + 0.002*\"bon\" + 0.002*\"petit\" + 0.002*\"imagine\" + 0.002*\"spécialite\" + 0.002*\"concerné\"\n",
      "Topic: 6 Word: 0.002*\"d\" + 0.002*\"situation\" + 0.002*\"  \" + 0.002*\"national\" + 0.002*\"guerre\" + 0.002*\"critère\" + 0.002*\"famine\" + 0.002*\"vacciner\" + 0.002*\"mieux\" + 0.002*\"organisme\"\n",
      "Topic: 7 Word: 0.003*\"partout\" + 0.002*\"administratif\" + 0.002*\"grave\" + 0.002*\"politique\" + 0.002*\"Lyme\" + 0.002*\"objet\" + 0.002*\"autorité\" + 0.002*\"gestion\" + 0.002*\"rappeler\" + 0.002*\"cotiser\"\n",
      "Topic: 8 Word: 0.003*\"partir\" + 0.002*\"cancer\" + 0.002*\"pollution\" + 0.002*\"  \" + 0.002*\"besoin\" + 0.002*\"eau\" + 0.002*\"menacer\" + 0.002*\"alimentation\" + 0.002*\"adapter\" + 0.002*\"an\"\n",
      "Topic: 9 Word: 0.005*\"d\" + 0.003*\"l\" + 0.002*\"vie\" + 0.002*\"y\" + 0.002*\"enfant\" + 0.002*\"grossesse\" + 0.002*\"migration\" + 0.002*\"falloir\" + 0.002*\"cas\" + 0.002*\"faire\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize api instance\n",
    "auth = tw.OAuthHandler(consumer_key='suAN7sV4kjo6rNLQsfuNBzQzu',\n",
    "                        consumer_secret='shj6l92LoN4RkoaFM0Naap0cBW1MbtFAtUKFs45NRNIXELF1jY')\n",
    "\n",
    "auth.set_access_token('1259561442149351426-gHA05KhC5Oj16kppzcXt5JPhmruRDw',\n",
    "                        'dKi2yJfcBRgM65ZUHVzJi85ZzlGfd7gQiGUGQHZ3iOjZJ')\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords=[\"pandemie\",\"pandémie\",\"epidemie\",\"epidémie\",\"sras\",\"coronavirus\"]\n",
    "OU=\" OR \"\n",
    "search_words= OU.join([\"#\"+x for x in swords])\n",
    "search_words+=\" AND (@gouvernementFR OR @Elysee OR @EmmanuelMacron)\"\n",
    "search_words+=\" -filter:links\"\n",
    "search_words+=\" AND -filter:retweets\"\n",
    "search_words+=\" AND -filter:replies\"\n",
    "search_date=\"2020-07-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"fr\",\n",
    "              since=search_date, tweet_mode='extended').items(3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pandemie OR #pandémie OR #epidemie OR #epidémie OR #sras OR #coronavirus AND (@gouvernementFR OR @Elysee OR @EmmanuelMacron) -filter:links AND -filter:retweets AND -filter:replies\n"
     ]
    }
   ],
   "source": [
    "print(search_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_recup(tweets):\n",
    "    tweet_list=[]\n",
    "    for tweet in tweets:\n",
    "        tweet_text=tweet.full_text\n",
    "        tweed_id_str=tweet.id_str\n",
    "        tweet_created_at=tweet.created_at\n",
    "        tweet_user_location=tweet.user.location\n",
    "        tweet_list.append({\"tweet_text\":tweet_text,\n",
    "                         \"tweed_id_str\":tweed_id_str,\n",
    "                         \"tweet_created_at\":tweet_created_at,\n",
    "                         \"tweet_user_location\":tweet_user_location})\n",
    "    tweets_df=pd.DataFrame(tweet_list,columns=[\"tweet_text\",\"tweed_id_str\",\"tweet_created_at\",\"tweet_user_location\"])\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=tweet_recup(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"tweets.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###On met ce bloc de côté pour l'instant\n",
    "\n",
    "\"\"\"def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    program_start = time.time()\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"fr\", since=search_date, tweet_mode='extended').items(numTweets)\n",
    "# Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "# Obtain the following info (methods to call them out):\n",
    "        # user.screen_name - twitter handle\n",
    "        # user.description - description of account\n",
    "        # user.location - where is he tweeting from\n",
    "        # user.friends_count - no. of other users that user is following (following)\n",
    "        # user.followers_count - no. of other users who are following this user (followers)\n",
    "        # user.statuses_count - total tweets by user\n",
    "        # user.created_at - when the user account was created\n",
    "        # created_at - when the tweet was created\n",
    "        # retweet_count - no. of retweets\n",
    "        # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "        # retweeted_status.full_text - full text of the tweet\n",
    "        # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "# Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "for tweet in tweet_list:\n",
    "# Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "# Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "# Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "# increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "        \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "        time.sleep(920) #15 minute sleep time\n",
    "        \n",
    "###Cette partie je n'en ai à priori pas besoin###\n",
    "# Once all runs have completed, save them to a single csv file:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "# Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_sahkprotests_tweets.csv'\n",
    "# Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
